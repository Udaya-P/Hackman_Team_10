{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML Hackathon: HMM and Reinforcement Learning Word Completion\n",
        "\n",
        "## Problem Statement\n",
        "This notebook implements:\n",
        "1. **Part 1**: Hidden Markov Model for letter probability estimation\n",
        "2. **Part 2**: Reinforcement Learning agent using HMM guidance"
      ],
      "metadata": {
        "id": "zzZaEk-UOpeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Packages\n",
        "Install core dependencies like hmmlearn used for training Hidden Markov Models."
      ],
      "metadata": {
        "id": "LqVGRkaAZHi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0_ei2EYOk6K",
        "outputId": "16534845-0a29-4d38-b57c-b76cacae7c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n",
            "Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/166.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install hmmlearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 1: Hidden Markov Model Implementation"
      ],
      "metadata": {
        "id": "cvz82p73Zhnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a Hidden Markov Model (HMM) system that learns letter patterns in words, grouped by their lengths.\n",
        "It trains a separate HMM for each word length using a text corpus, then predicts the most likely next letter in a masked word by combining HMM emission probabilities with word frequency information."
      ],
      "metadata": {
        "id": "jveMIz17ZkQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pickle\n",
        "from hmmlearn.hmm import MultinomialHMM\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "A2I = {c:i for i,c in enumerate(alphabet)}\n",
        "I2A = {i:c for c,i in A2I.items()}\n",
        "N = len(alphabet)\n",
        "\n",
        "# Convert word â†’ sequence of one-hot vectors (expected by new hmmlearn)\n",
        "def encode_word(word):\n",
        "    seq = np.zeros((len(word), N), dtype=int)\n",
        "    for i, c in enumerate(word):\n",
        "        seq[i, A2I[c]] = 1\n",
        "    return seq\n",
        "\n",
        "\n",
        "class PerLengthHMMs:\n",
        "    def __init__(self):\n",
        "        self.hmm_by_length = {}\n",
        "        self.words_by_length = {}\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, \"wb\") as f:\n",
        "            pickle.dump(self, f)\n",
        "        print(f\"âœ… Saved model to {path}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        with open(path, \"rb\") as f:\n",
        "            print(f\"âœ… Loaded model from {path}\")\n",
        "            return pickle.load(f)\n",
        "\n",
        "    def train(self, corpus_path, states_per_pos=3, max_len=25):\n",
        "        print(\"ğŸ“Œ Loading corpus...\")\n",
        "        with open(corpus_path, 'r') as f:\n",
        "            words = [w.strip().lower() for w in f if w.strip().isalpha()]\n",
        "\n",
        "        print(f\"ğŸ“Œ {len(words)} words loaded.\")\n",
        "\n",
        "        # group by length\n",
        "        for w in words:\n",
        "            if len(w) <= max_len:\n",
        "                self.words_by_length.setdefault(len(w), []).append(w)\n",
        "\n",
        "        for L, word_list in sorted(self.words_by_length.items()):\n",
        "            print(f\"\\nğŸ¯ Training HMM for length {L} ({len(word_list)} words)...\")\n",
        "\n",
        "            n_states = max(2, states_per_pos * L)\n",
        "\n",
        "            model = MultinomialHMM(\n",
        "                n_components=n_states,\n",
        "                n_iter=15,\n",
        "                tol=1e-4,\n",
        "                init_params=\"\",\n",
        "                params=\"ste\"\n",
        "            )\n",
        "\n",
        "            model.n_features = N\n",
        "\n",
        "            # Initialize parameters\n",
        "            model.startprob_ = np.full(n_states, 1/n_states)\n",
        "            model.transmat_  = np.full((n_states, n_states), 1/n_states)\n",
        "            model.emissionprob_ = np.full((n_states, N), 1/N)\n",
        "\n",
        "            # Build training matrix\n",
        "            X = np.vstack([encode_word(w) for w in word_list])\n",
        "            lengths = [len(w) for w in word_list]\n",
        "\n",
        "            model.fit(X, lengths)\n",
        "            self.hmm_by_length[L] = model\n",
        "\n",
        "        print(\"\\nâœ… Training complete!\")\n",
        "\n",
        "    def _candidate_words(self, masked, guessed):\n",
        "        L = len(masked)\n",
        "        if L not in self.words_by_length:\n",
        "            return []\n",
        "        return [\n",
        "            w for w in self.words_by_length[L]\n",
        "            if all((m == \"_\" or w[i] == m) and (w[i] not in guessed or m == w[i])\n",
        "                   for i, m in enumerate(masked))\n",
        "        ]\n",
        "\n",
        "    def predict_letter_distribution(self, masked, guessed):\n",
        "        L = len(masked)\n",
        "        if L not in self.hmm_by_length:\n",
        "            return None\n",
        "\n",
        "        hmm = self.hmm_by_length[L]\n",
        "\n",
        "        # HMM-based letter weights (average emission)\n",
        "        p_hmm = hmm.emissionprob_.mean(axis=0)\n",
        "\n",
        "        for g in guessed:\n",
        "            p_hmm[A2I[g]] = 0\n",
        "\n",
        "        # Normalize\n",
        "        s = p_hmm.sum()\n",
        "        if s > 0:\n",
        "            p_hmm = p_hmm / s\n",
        "        else:\n",
        "            p_hmm = np.ones(N) / N\n",
        "\n",
        "        p_hmm = {I2A[i]: p_hmm[i] for i in range(N)}\n",
        "\n",
        "        # Word frequency weighting\n",
        "        candidates = self._candidate_words(masked, guessed)\n",
        "        freq = Counter()\n",
        "        for w in candidates:\n",
        "            for i, c in enumerate(masked):\n",
        "                if c == \"_\":\n",
        "                    freq[w[i]] += 1\n",
        "        total = sum(freq.values()) + 1e-9\n",
        "        p_freq = {c: freq[c]/total for c in alphabet}\n",
        "\n",
        "        # Blend both\n",
        "        ALPHA = 0.6\n",
        "        final = {c: ALPHA*p_hmm.get(c,0) + (1-ALPHA)*p_freq.get(c,0) for c in alphabet}\n",
        "\n",
        "        # Normalize final distribution\n",
        "        S = sum(final.values()) + 1e-9\n",
        "        final = {c: final[c]/S for c in final}\n",
        "\n",
        "        return final\n",
        "\n",
        "    def predict_letter(self, masked, guessed):\n",
        "        dist = self.predict_letter_distribution(masked, guessed)\n",
        "        return max(dist.items(), key=lambda x: x[1])[0]\n"
      ],
      "metadata": {
        "id": "G_vKar_nPK-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates an instance of the PerLengthHMMs class, trains it on the given word corpus to learn HMMs for different word lengths, and then saves the trained model to a file for later use."
      ],
      "metadata": {
        "id": "HpsZZeCNZ0CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PerLengthHMMs()\n",
        "model.train(\"/content/corpus.txt\")\n",
        "model.save(\"/content/per_length_hmms.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYZAW8wLPMgA",
        "outputId": "9ce04549-8e5f-40be-d331-5545837f283d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n",
            "WARNING:hmmlearn.base:Some rows of transmat_ have zero sum because no transition from the state was ever observed.\n",
            "WARNING:hmmlearn.base:Some rows of transmat_ have zero sum because no transition from the state was ever observed.\n",
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n",
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Œ Loading corpus...\n",
            "ğŸ“Œ 45701 words loaded.\n",
            "\n",
            "ğŸ¯ Training HMM for length 1 (23 words)...\n",
            "\n",
            "ğŸ¯ Training HMM for length 2 (33 words)...\n",
            "\n",
            "ğŸ¯ Training HMM for length 3 (246 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 4 (937 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 5 (1895 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 6 (3268 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 7 (4485 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 8 (5714 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 9 (6234 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 10 (6065 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 11 (5158 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 12 (4037 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 13 (3012 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 14 (1984 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 15 (1195 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 16 (703 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 17 (375 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 18 (176 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 19 (90 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 20 (42 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 21 (17 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n",
            "WARNING:hmmlearn.base:Fitting a model with 6005 free scalar parameters with only 4576 data points will result in a degenerate solution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 22 (8 words)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n",
            "WARNING:hmmlearn.base:Fitting a model with 6485 free scalar parameters with only 1794 data points will result in a degenerate solution.\n",
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n",
            "WARNING:hmmlearn.base:Fitting a model with 6983 free scalar parameters with only 624 data points will result in a degenerate solution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Training HMM for length 23 (3 words)...\n",
            "\n",
            "ğŸ¯ Training HMM for length 24 (1 words)...\n",
            "\n",
            "âœ… Training complete!\n",
            "âœ… Saved model to /content/new_per_length_hmms.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 2: Reinforcement Learning Implementation"
      ],
      "metadata": {
        "id": "9F3G0m54aD2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds and trains a Deep Q-Network (DQN) reinforcement learning agent to play Hangman using the patterns learned by the HMM.\n",
        "It loads the pre-trained HMM to guide letter prediction, defines a game environment and reward system, and then trains the DQN over thousands of episodes to maximize correct guesses and minimize wrong ones."
      ],
      "metadata": {
        "id": "sE-ULZN_aQv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As the HMM has been trained now we are focusing on building the RL using DQN.\n",
        "!pip install torch torchvision\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =====================\n",
        "# CONFIG\n",
        "# =====================\n",
        "MODEL_PATH = \"/content/per_length_hmms.pkl\"\n",
        "MAX_LEN = 25\n",
        "MAX_LIVES = 6\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "NUM_EPISODES = 10000\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "LR = 1e-4\n",
        "BUFFER_CAPACITY = 150000\n",
        "TARGET_UPDATE_FREQ = 600\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 0.9993\n",
        "HIDDEN = 256\n",
        "\n",
        "# =====================\n",
        "# LOAD HMM\n",
        "# =====================\n",
        "with open(MODEL_PATH, \"rb\") as f:\n",
        "    trainer = pickle.load(f)\n",
        "\n",
        "print(\"âœ… Loaded HMM trainer\")\n",
        "\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "V = len(alphabet)\n",
        "A2I = {c:i for i,c in enumerate(alphabet)}\n",
        "I2A = {i:c for c,i in A2I.items()}\n",
        "\n",
        "def get_hmm_letter_agg(masked, guessed, trainer):\n",
        "    dist = trainer.predict_letter_distribution(masked, guessed)\n",
        "    if dist is None:\n",
        "        vec = np.ones(V)/V\n",
        "    else:\n",
        "        vec = np.array([dist.get(c,0) for c in alphabet], dtype=np.float32)\n",
        "    for g in guessed:\n",
        "        if g in A2I:\n",
        "            vec[A2I[g]] = 0\n",
        "    s = vec.sum()\n",
        "    return (vec/s if s>0 else np.ones(V)/V).astype(np.float32)\n",
        "\n",
        "# =====================\n",
        "# ENVIRONMENT\n",
        "# =====================\n",
        "class HangmanEnv:\n",
        "    def __init__(self, trainer, max_len=MAX_LEN, max_lives=MAX_LIVES):\n",
        "        self.trainer = trainer\n",
        "        self.max_len = max_len\n",
        "        self.max_lives = max_lives\n",
        "\n",
        "    def reset(self, word=None):\n",
        "        if word:\n",
        "            self.word = word\n",
        "        else:\n",
        "            L = random.choice(list(self.trainer.words_by_length.keys()))\n",
        "            self.word = random.choice(self.trainer.words_by_length[L])\n",
        "        self.masked = \"_\" * len(self.word)\n",
        "        self.guessed = set()\n",
        "        self.wrong = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        pos = np.zeros(self.max_len * V, np.float32)\n",
        "        for i, ch in enumerate(self.masked):\n",
        "            if ch != \"_\":\n",
        "                pos[i*V + A2I[ch]] = 1\n",
        "        guessed_vec = np.zeros(V, np.float32)\n",
        "        for g in self.guessed:\n",
        "            guessed_vec[A2I[g]] = 1\n",
        "        hmm_vec = get_hmm_letter_agg(self.masked, self.guessed, self.trainer)\n",
        "        life = np.array([(self.max_lives - self.wrong) / self.max_lives], np.float32)\n",
        "        return np.concatenate([pos, guessed_vec, hmm_vec, life])\n",
        "\n",
        "    def valid_actions_mask(self):\n",
        "        m = np.ones(V, bool)\n",
        "        for g in self.guessed:\n",
        "            m[A2I[g]] = False\n",
        "        return m\n",
        "\n",
        "    def step(self, action_idx):\n",
        "        letter = I2A[action_idx]\n",
        "        reward = 0\n",
        "        done = False\n",
        "\n",
        "        # Reward policy which has been implemented (Reviewer keep this in mind)\n",
        "        if letter in self.guessed:\n",
        "            reward -= 3\n",
        "        else:\n",
        "            self.guessed.add(letter)\n",
        "            if letter in self.word:\n",
        "                self.masked = \"\".join(\n",
        "                    letter if self.word[i] == letter else self.masked[i]\n",
        "                    for i in range(len(self.word)))\n",
        "                reward += 2\n",
        "                if \"_\" not in self.masked:\n",
        "                    reward += 10\n",
        "                    done = True\n",
        "            else:\n",
        "                self.wrong += 1\n",
        "                reward -= 1\n",
        "                if self.wrong >= self.max_lives:\n",
        "                    reward -= 10\n",
        "                    done = True\n",
        "\n",
        "        return self._get_state(), reward, done, {}\n",
        "\n",
        "# =====================\n",
        "# DQN\n",
        "# =====================\n",
        "state_dim = MAX_LEN*V + V + V + 1\n",
        "action_dim = V\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, s, a):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(s, HIDDEN), nn.ReLU(),\n",
        "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
        "            nn.Linear(HIDDEN, a))\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, cap): self.buf = deque(maxlen=cap)\n",
        "    def push(self, *ex): self.buf.append(ex)\n",
        "    def sample(self, n):\n",
        "        batch = random.sample(self.buf, n)\n",
        "        return map(np.stack, zip(*batch))\n",
        "    def __len__(self): return len(self.buf)\n",
        "\n",
        "env = HangmanEnv(trainer)\n",
        "policy = DQN(state_dim, action_dim).to(DEVICE)\n",
        "target = DQN(state_dim, action_dim).to(DEVICE)\n",
        "target.load_state_dict(policy.state_dict())\n",
        "optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
        "buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
        "\n",
        "eps = EPS_START\n",
        "steps = 0\n",
        "\n",
        "print(\"ğŸš€ Training DQN...\")\n",
        "\n",
        "for ep in range(NUM_EPISODES):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        mask = env.valid_actions_mask()\n",
        "\n",
        "        if random.random() < eps:\n",
        "            action = random.choice(np.where(mask)[0])\n",
        "        else:\n",
        "            s = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
        "            q = policy(s).detach().cpu().numpy().squeeze()\n",
        "            q[~mask] = -1e9\n",
        "            action = int(np.argmax(q))\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        buffer.push(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        if len(buffer) >= BATCH_SIZE:\n",
        "            s,a,r,s2,d = buffer.sample(BATCH_SIZE)\n",
        "            s = torch.tensor(s, dtype=torch.float32, device=DEVICE)\n",
        "            a = torch.tensor(a, dtype=torch.long, device=DEVICE).unsqueeze(1)\n",
        "            r = torch.tensor(r, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
        "            s2 = torch.tensor(s2, dtype=torch.float32, device=DEVICE)\n",
        "            d = torch.tensor(d, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
        "\n",
        "            q_vals = policy(s).gather(1, a)\n",
        "            with torch.no_grad():\n",
        "                q_next = target(s2)\n",
        "                guessed = s2[:, MAX_LEN*V:MAX_LEN*V+V]\n",
        "                mask2 = (guessed < 0.5)\n",
        "                q_next[~mask2] = -1e9\n",
        "                max_q = q_next.max(dim=1, keepdim=True)[0]\n",
        "                target_q = r + (1 - d) * GAMMA * max_q\n",
        "\n",
        "            loss = nn.functional.mse_loss(q_vals, target_q)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        steps += 1\n",
        "        eps = max(EPS_END, eps * EPS_DECAY)\n",
        "\n",
        "        if steps % TARGET_UPDATE_FREQ == 0:\n",
        "            target.load_state_dict(policy.state_dict())\n",
        "\n",
        "    if ep % 200 == 0:\n",
        "        print(f\"Ep {ep}/{NUM_EPISODES} | eps={eps:.3f}\")\n",
        "\n",
        "torch.save(policy.state_dict(), \"dqn_hangman_policy.pth\")\n",
        "print(\"âœ… Training Complete â€” Saved: dqn_hangman_policy.pth\")\n",
        "# As mentioned the same policy will be used later to get the total marks for the test set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abZNjISsudbn",
        "outputId": "32bbea70-9e79-49f6-a172-cdb5edbde1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Using device: cuda\n",
            "âœ… Loaded HMM trainer\n",
            "ğŸš€ Training DQN...\n",
            "Ep 0/10000 | eps=0.994\n",
            "Ep 200/10000 | eps=0.216\n",
            "Ep 400/10000 | eps=0.050\n",
            "Ep 600/10000 | eps=0.050\n",
            "Ep 800/10000 | eps=0.050\n",
            "Ep 1000/10000 | eps=0.050\n",
            "Ep 1200/10000 | eps=0.050\n",
            "Ep 1400/10000 | eps=0.050\n",
            "Ep 1600/10000 | eps=0.050\n",
            "Ep 1800/10000 | eps=0.050\n",
            "Ep 2000/10000 | eps=0.050\n",
            "Ep 2200/10000 | eps=0.050\n",
            "Ep 2400/10000 | eps=0.050\n",
            "Ep 2600/10000 | eps=0.050\n",
            "Ep 2800/10000 | eps=0.050\n",
            "Ep 3000/10000 | eps=0.050\n",
            "Ep 3200/10000 | eps=0.050\n",
            "Ep 3400/10000 | eps=0.050\n",
            "Ep 3600/10000 | eps=0.050\n",
            "Ep 3800/10000 | eps=0.050\n",
            "Ep 4000/10000 | eps=0.050\n",
            "Ep 4200/10000 | eps=0.050\n",
            "Ep 4400/10000 | eps=0.050\n",
            "Ep 4600/10000 | eps=0.050\n",
            "Ep 4800/10000 | eps=0.050\n",
            "Ep 5000/10000 | eps=0.050\n",
            "Ep 5200/10000 | eps=0.050\n",
            "Ep 5400/10000 | eps=0.050\n",
            "Ep 5600/10000 | eps=0.050\n",
            "Ep 5800/10000 | eps=0.050\n",
            "Ep 6000/10000 | eps=0.050\n",
            "Ep 6200/10000 | eps=0.050\n",
            "Ep 6400/10000 | eps=0.050\n",
            "Ep 6600/10000 | eps=0.050\n",
            "Ep 6800/10000 | eps=0.050\n",
            "Ep 7000/10000 | eps=0.050\n",
            "Ep 7200/10000 | eps=0.050\n",
            "Ep 7400/10000 | eps=0.050\n",
            "Ep 7600/10000 | eps=0.050\n",
            "Ep 7800/10000 | eps=0.050\n",
            "Ep 8000/10000 | eps=0.050\n",
            "Ep 8200/10000 | eps=0.050\n",
            "Ep 8400/10000 | eps=0.050\n",
            "Ep 8600/10000 | eps=0.050\n",
            "Ep 8800/10000 | eps=0.050\n",
            "Ep 9000/10000 | eps=0.050\n",
            "Ep 9200/10000 | eps=0.050\n",
            "Ep 9400/10000 | eps=0.050\n",
            "Ep 9600/10000 | eps=0.050\n",
            "Ep 9800/10000 | eps=0.050\n",
            "âœ… Training Complete â€” Saved: dqn_hangman_policy.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function evaluates the trained DQN Hangman agent on a set of test words to measure its performance.\n",
        "It simulates games for each word, logs the guesses and results for a few examples, and finally reports metrics like win rate, average wrong guesses, and total reward."
      ],
      "metadata": {
        "id": "Mnbiw2Qmaetz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_and_show(policy, trainer, test_path, max_show=15):\n",
        "    env = HangmanEnv(trainer)\n",
        "    policy.eval()\n",
        "\n",
        "    with open(test_path) as f:\n",
        "        test_words = [w.strip() for w in f if len(w.strip()) > 0]\n",
        "\n",
        "    wins = 0\n",
        "    total_wrong = 0\n",
        "    total_repeats = 0\n",
        "    total_reward = 0\n",
        "\n",
        "    print(f\"\\nğŸ” Evaluating on {len(test_words)} words...\")\n",
        "    print(\"Showing first\", max_show, \"words:\\n\")\n",
        "\n",
        "    for idx, word in enumerate(test_words):\n",
        "        state = env.reset(word=word)\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        if idx < max_show:\n",
        "            print(\"----------------------------------------------------\")\n",
        "            print(f\"ğŸ”¤ Target Word: {word}\")\n",
        "            print(f\"Start: {env.masked}\")\n",
        "\n",
        "        while not done:\n",
        "            mask = env.valid_actions_mask()\n",
        "\n",
        "            s = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
        "            q = policy(s).cpu().detach().numpy().squeeze()\n",
        "            q[~mask] = -1e9\n",
        "            action = int(np.argmax(q))\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if idx < max_show:\n",
        "                print(f\"Guess: {I2A[action]} | Masked: {env.masked} | Wrong: {env.wrong}\", end=\"\")\n",
        "                if info.get(\"repeated\", False):\n",
        "                    print(\"  (repeated)\", end=\"\")\n",
        "                print()\n",
        "\n",
        "            if info.get(\"repeated\", False):\n",
        "                total_repeats += 1\n",
        "\n",
        "        if env.masked == env.word:\n",
        "            wins += 1\n",
        "        total_wrong += env.wrong\n",
        "        total_reward += episode_reward\n",
        "\n",
        "    # Final Stats\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\"Test Words: {len(test_words)}\")\n",
        "    print(f\"Win Rate: {wins/len(test_words)*100:.2f}%\")\n",
        "    print(f\"Avg Wrong Guesses: {total_wrong/len(test_words):.2f}\")\n",
        "    print(f\"Final Score (sum of rewards): {total_reward:.2f}\") # from the previous policy\n",
        "    print(\"==============================\")\n"
      ],
      "metadata": {
        "id": "npgK5WnEyJMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the previously trained DQN model and runs it on a test dataset to evaluate its performance.."
      ],
      "metadata": {
        "id": "7uDzc5kWaoGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = DQN(state_dim, action_dim).to(DEVICE)\n",
        "policy.load_state_dict(torch.load(\"/content/dqn_hangman_policy.pth\"))\n",
        "\n",
        "evaluate_and_show(policy, trainer, \"/content/test.txt\", max_show=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t80BBYmz0Qn",
        "outputId": "05920131-c719-4bb4-960d-3bcd6605bbbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Evaluating on 2000 words...\n",
            "Showing first 15 words:\n",
            "\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: marmar\n",
            "Start: ______\n",
            "Guess: e | Masked: ______ | Wrong: 1\n",
            "Guess: s | Masked: ______ | Wrong: 2\n",
            "Guess: a | Masked: _a__a_ | Wrong: 2\n",
            "Guess: r | Masked: _ar_ar | Wrong: 2\n",
            "Guess: m | Masked: marmar | Wrong: 2\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: janet\n",
            "Start: _____\n",
            "Guess: e | Masked: ___e_ | Wrong: 0\n",
            "Guess: r | Masked: ___e_ | Wrong: 1\n",
            "Guess: s | Masked: ___e_ | Wrong: 2\n",
            "Guess: a | Masked: _a_e_ | Wrong: 2\n",
            "Guess: t | Masked: _a_et | Wrong: 2\n",
            "Guess: c | Masked: _a_et | Wrong: 3\n",
            "Guess: w | Masked: _a_et | Wrong: 4\n",
            "Guess: n | Masked: _anet | Wrong: 4\n",
            "Guess: k | Masked: _anet | Wrong: 5\n",
            "Guess: z | Masked: _anet | Wrong: 6\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: dentistical\n",
            "Start: ___________\n",
            "Guess: t | Masked: ___t__t____ | Wrong: 0\n",
            "Guess: e | Masked: _e_t__t____ | Wrong: 0\n",
            "Guess: s | Masked: _e_t_st____ | Wrong: 0\n",
            "Guess: v | Masked: _e_t_st____ | Wrong: 1\n",
            "Guess: r | Masked: _e_t_st____ | Wrong: 2\n",
            "Guess: o | Masked: _e_t_st____ | Wrong: 3\n",
            "Guess: i | Masked: _e_tisti___ | Wrong: 3\n",
            "Guess: a | Masked: _e_tisti_a_ | Wrong: 3\n",
            "Guess: c | Masked: _e_tistica_ | Wrong: 3\n",
            "Guess: y | Masked: _e_tistica_ | Wrong: 4\n",
            "Guess: n | Masked: _entistica_ | Wrong: 4\n",
            "Guess: d | Masked: dentistica_ | Wrong: 4\n",
            "Guess: l | Masked: dentistical | Wrong: 4\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: troveless\n",
            "Start: _________\n",
            "Guess: e | Masked: ____e_e__ | Wrong: 0\n",
            "Guess: s | Masked: ____e_ess | Wrong: 0\n",
            "Guess: r | Masked: _r__e_ess | Wrong: 0\n",
            "Guess: o | Masked: _ro_e_ess | Wrong: 0\n",
            "Guess: t | Masked: tro_e_ess | Wrong: 0\n",
            "Guess: l | Masked: tro_eless | Wrong: 0\n",
            "Guess: v | Masked: troveless | Wrong: 0\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: unnotify\n",
            "Start: ________\n",
            "Guess: e | Masked: ________ | Wrong: 1\n",
            "Guess: s | Masked: ________ | Wrong: 2\n",
            "Guess: a | Masked: ________ | Wrong: 3\n",
            "Guess: i | Masked: _____i__ | Wrong: 3\n",
            "Guess: p | Masked: _____i__ | Wrong: 4\n",
            "Guess: o | Masked: ___o_i__ | Wrong: 4\n",
            "Guess: n | Masked: _nno_i__ | Wrong: 4\n",
            "Guess: t | Masked: _nnoti__ | Wrong: 4\n",
            "Guess: f | Masked: _nnotif_ | Wrong: 4\n",
            "Guess: u | Masked: unnotif_ | Wrong: 4\n",
            "Guess: y | Masked: unnotify | Wrong: 4\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: gastrostenosis\n",
            "Start: ______________\n",
            "Guess: t | Masked: ___t___t______ | Wrong: 0\n",
            "Guess: s | Masked: __st__st___s_s | Wrong: 0\n",
            "Guess: o | Masked: __st_ost__os_s | Wrong: 0\n",
            "Guess: n | Masked: __st_ost_nos_s | Wrong: 0\n",
            "Guess: i | Masked: __st_ost_nosis | Wrong: 0\n",
            "Guess: u | Masked: __st_ost_nosis | Wrong: 1\n",
            "Guess: g | Masked: g_st_ost_nosis | Wrong: 1\n",
            "Guess: c | Masked: g_st_ost_nosis | Wrong: 2\n",
            "Guess: f | Masked: g_st_ost_nosis | Wrong: 3\n",
            "Guess: r | Masked: g_strost_nosis | Wrong: 3\n",
            "Guess: a | Masked: gastrost_nosis | Wrong: 3\n",
            "Guess: e | Masked: gastrostenosis | Wrong: 3\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: preaffiliation\n",
            "Start: ______________\n",
            "Guess: t | Masked: __________t___ | Wrong: 0\n",
            "Guess: s | Masked: __________t___ | Wrong: 1\n",
            "Guess: m | Masked: __________t___ | Wrong: 2\n",
            "Guess: r | Masked: _r________t___ | Wrong: 2\n",
            "Guess: e | Masked: _re_______t___ | Wrong: 2\n",
            "Guess: i | Masked: _re___i_i_ti__ | Wrong: 2\n",
            "Guess: o | Masked: _re___i_i_tio_ | Wrong: 2\n",
            "Guess: l | Masked: _re___ili_tio_ | Wrong: 2\n",
            "Guess: f | Masked: _re_ffili_tio_ | Wrong: 2\n",
            "Guess: b | Masked: _re_ffili_tio_ | Wrong: 3\n",
            "Guess: c | Masked: _re_ffili_tio_ | Wrong: 4\n",
            "Guess: a | Masked: _reaffiliatio_ | Wrong: 4\n",
            "Guess: g | Masked: _reaffiliatio_ | Wrong: 5\n",
            "Guess: n | Masked: _reaffiliation | Wrong: 5\n",
            "Guess: p | Masked: preaffiliation | Wrong: 5\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: obpyriform\n",
            "Start: __________\n",
            "Guess: t | Masked: __________ | Wrong: 1\n",
            "Guess: s | Masked: __________ | Wrong: 2\n",
            "Guess: a | Masked: __________ | Wrong: 3\n",
            "Guess: e | Masked: __________ | Wrong: 4\n",
            "Guess: o | Masked: o______o__ | Wrong: 4\n",
            "Guess: p | Masked: o_p____o__ | Wrong: 4\n",
            "Guess: i | Masked: o_p__i_o__ | Wrong: 4\n",
            "Guess: n | Masked: o_p__i_o__ | Wrong: 5\n",
            "Guess: r | Masked: o_p_ri_or_ | Wrong: 5\n",
            "Guess: y | Masked: o_pyri_or_ | Wrong: 5\n",
            "Guess: l | Masked: o_pyri_or_ | Wrong: 6\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: veratrinize\n",
            "Start: ___________\n",
            "Guess: t | Masked: ____t______ | Wrong: 0\n",
            "Guess: s | Masked: ____t______ | Wrong: 1\n",
            "Guess: o | Masked: ____t______ | Wrong: 2\n",
            "Guess: e | Masked: _e__t_____e | Wrong: 2\n",
            "Guess: a | Masked: _e_at_____e | Wrong: 2\n",
            "Guess: r | Masked: _eratr____e | Wrong: 2\n",
            "Guess: d | Masked: _eratr____e | Wrong: 3\n",
            "Guess: i | Masked: _eratri_i_e | Wrong: 3\n",
            "Guess: z | Masked: _eratri_ize | Wrong: 3\n",
            "Guess: v | Masked: veratri_ize | Wrong: 3\n",
            "Guess: n | Masked: veratrinize | Wrong: 3\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: protection\n",
            "Start: __________\n",
            "Guess: t | Masked: ___t__t___ | Wrong: 0\n",
            "Guess: e | Masked: ___te_t___ | Wrong: 0\n",
            "Guess: s | Masked: ___te_t___ | Wrong: 1\n",
            "Guess: o | Masked: __ote_t_o_ | Wrong: 1\n",
            "Guess: p | Masked: p_ote_t_o_ | Wrong: 1\n",
            "Guess: c | Masked: p_otect_o_ | Wrong: 1\n",
            "Guess: n | Masked: p_otect_on | Wrong: 1\n",
            "Guess: r | Masked: protect_on | Wrong: 1\n",
            "Guess: i | Masked: protection | Wrong: 1\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: guileless\n",
            "Start: _________\n",
            "Guess: e | Masked: ____e_e__ | Wrong: 0\n",
            "Guess: s | Masked: ____e_ess | Wrong: 0\n",
            "Guess: r | Masked: ____e_ess | Wrong: 1\n",
            "Guess: f | Masked: ____e_ess | Wrong: 2\n",
            "Guess: t | Masked: ____e_ess | Wrong: 3\n",
            "Guess: o | Masked: ____e_ess | Wrong: 4\n",
            "Guess: a | Masked: ____e_ess | Wrong: 5\n",
            "Guess: l | Masked: ___leless | Wrong: 5\n",
            "Guess: u | Masked: _u_leless | Wrong: 5\n",
            "Guess: i | Masked: _uileless | Wrong: 5\n",
            "Guess: g | Masked: guileless | Wrong: 5\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: phototactic\n",
            "Start: ___________\n",
            "Guess: t | Masked: ___t_t__t__ | Wrong: 0\n",
            "Guess: m | Masked: ___t_t__t__ | Wrong: 1\n",
            "Guess: c | Masked: ___t_t_ct_c | Wrong: 1\n",
            "Guess: o | Masked: __otot_ct_c | Wrong: 1\n",
            "Guess: h | Masked: _hotot_ct_c | Wrong: 1\n",
            "Guess: r | Masked: _hotot_ct_c | Wrong: 2\n",
            "Guess: i | Masked: _hotot_ctic | Wrong: 2\n",
            "Guess: s | Masked: _hotot_ctic | Wrong: 3\n",
            "Guess: e | Masked: _hotot_ctic | Wrong: 4\n",
            "Guess: d | Masked: _hotot_ctic | Wrong: 5\n",
            "Guess: a | Masked: _hototactic | Wrong: 5\n",
            "Guess: p | Masked: phototactic | Wrong: 5\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: triloculate\n",
            "Start: ___________\n",
            "Guess: t | Masked: t________t_ | Wrong: 0\n",
            "Guess: s | Masked: t________t_ | Wrong: 1\n",
            "Guess: a | Masked: t_______at_ | Wrong: 1\n",
            "Guess: b | Masked: t_______at_ | Wrong: 2\n",
            "Guess: i | Masked: t_i_____at_ | Wrong: 2\n",
            "Guess: r | Masked: tri_____at_ | Wrong: 2\n",
            "Guess: d | Masked: tri_____at_ | Wrong: 3\n",
            "Guess: e | Masked: tri_____ate | Wrong: 3\n",
            "Guess: o | Masked: tri_o___ate | Wrong: 3\n",
            "Guess: l | Masked: trilo__late | Wrong: 3\n",
            "Guess: u | Masked: trilo_ulate | Wrong: 3\n",
            "Guess: c | Masked: triloculate | Wrong: 3\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: trustee\n",
            "Start: _______\n",
            "Guess: e | Masked: _____ee | Wrong: 0\n",
            "Guess: r | Masked: _r___ee | Wrong: 0\n",
            "Guess: t | Masked: tr__tee | Wrong: 0\n",
            "Guess: u | Masked: tru_tee | Wrong: 0\n",
            "Guess: s | Masked: trustee | Wrong: 0\n",
            "----------------------------------------------------\n",
            "ğŸ”¤ Target Word: sextole\n",
            "Start: _______\n",
            "Guess: e | Masked: _e____e | Wrong: 0\n",
            "Guess: o | Masked: _e__o_e | Wrong: 0\n",
            "Guess: r | Masked: _e__o_e | Wrong: 1\n",
            "Guess: s | Masked: se__o_e | Wrong: 1\n",
            "Guess: m | Masked: se__o_e | Wrong: 2\n",
            "Guess: p | Masked: se__o_e | Wrong: 3\n",
            "Guess: l | Masked: se__ole | Wrong: 3\n",
            "Guess: n | Masked: se__ole | Wrong: 4\n",
            "Guess: t | Masked: se_tole | Wrong: 4\n",
            "Guess: x | Masked: sextole | Wrong: 4\n",
            "\n",
            "==============================\n",
            "Test Words: 2000\n",
            "Win Rate: 71.05%\n",
            "Avg Wrong Guesses: 3.89\n",
            "Final Score (sum of rewards): 27955.00\n",
            "==============================\n"
          ]
        }
      ]
    }
  ]
}